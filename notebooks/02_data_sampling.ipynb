{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21a565f",
   "metadata": {},
   "source": [
    "# Data Sampling and DataLoaders for LLM Training\n",
    "\n",
    "When training Large Language Models (LLMs), we need to efficiently prepare and feed data to the model. This notebook demonstrates:\n",
    "\n",
    "1. **Why we need data sampling**: Raw text needs to be converted into training examples\n",
    "2. **Sliding window approach**: How to create overlapping sequences for better learning\n",
    "3. **DataLoader creation**: Batching sequences for efficient training\n",
    "4. **Input-target pairs**: Understanding the predict-next-token objective\n",
    "\n",
    "By the end, you'll understand how to transform raw text into batches of training data that LLMs can learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5c314",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we import the necessary libraries:\n",
    "- **PyTorch**: For tensor operations and data loading utilities\n",
    "- **tiktoken**: OpenAI's tokenizer for converting text to tokens\n",
    "- **Dataset & DataLoader**: PyTorch utilities for efficient data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e393ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to the path so we can import from core\n",
    "blog_dir = Path.cwd().parent\n",
    "sys.path.insert(0, str(blog_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd73886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97874738",
   "metadata": {},
   "source": [
    "## 2. Understanding the Sliding Window Dataset\n",
    "\n",
    "**The Problem**: LLMs are trained to predict the next token given previous tokens. We need to create many training examples from our text.\n",
    "\n",
    "**The Solution**: Use a sliding window approach where:\n",
    "- **max_length**: The number of tokens in each training sequence\n",
    "- **stride**: How many tokens to shift the window (controls overlap)\n",
    "- **Input**: A sequence of tokens\n",
    "- **Target**: The same sequence shifted by 1 position (next token prediction)\n",
    "\n",
    "For example, with tokens `[1, 2, 3, 4, 5]` and `max_length=3`:\n",
    "- Input: `[1, 2, 3]` → Target: `[2, 3, 4]`\n",
    "- Input: `[2, 3, 4]` → Target: `[3, 4, 5]`\n",
    "\n",
    "This teaches the model: given `[1, 2, 3]`, predict `2, 3, 4` at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e31a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the text into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8bb1e",
   "metadata": {},
   "source": [
    "## 3. Creating the DataLoader Function\n",
    "\n",
    "The `create_dataloader` function combines everything:\n",
    "1. Initializes the tokenizer (GPT-2's BPE tokenizer)\n",
    "2. Creates the dataset with sliding windows\n",
    "3. Wraps it in a DataLoader for batching and shuffling\n",
    "\n",
    "**Key Parameters**:\n",
    "- `batch_size`: Number of sequences to process together\n",
    "- `max_length`: Sequence length (context window)\n",
    "- `stride`: Window shift size (smaller = more overlap, more training examples)\n",
    "- `shuffle`: Randomize order (helps model generalize)\n",
    "- `drop_last`: Drop incomplete final batch (keeps batch sizes consistent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df353ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256, \n",
    "                      stride=128, shuffle=True, drop_last=True,\n",
    "                      num_workers=0):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for training language models\n",
    "    \n",
    "    Args:\n",
    "        txt: Raw text string to tokenize\n",
    "        batch_size: Number of sequences per batch\n",
    "        max_length: Length of each input sequence\n",
    "        stride: Step size for sliding window (smaller = more overlap)\n",
    "        shuffle: Whether to shuffle the data\n",
    "        drop_last: Whether to drop the last incomplete batch\n",
    "        num_workers: Number of worker processes for data loading\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader instance\n",
    "    \"\"\"\n",
    "    # Initialize the GPT-2 tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = SlidingWindowDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfcfc6",
   "metadata": {},
   "source": [
    "## 4. Load the Training Data\n",
    "\n",
    "Let's load our text data - Romeo and Juliet from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9590d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data '../data/romeo_juliet_gutenberg.txt' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/romeo_juliet_gutenberg.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "print(f\"Data '{file_path}' loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584372c2",
   "metadata": {},
   "source": [
    "## 5. Testing with Small Parameters\n",
    "\n",
    "Let's create a dataloader with small parameters to understand how it works:\n",
    "- `batch_size=1`: One sequence at a time\n",
    "- `max_length=4`: Only 4 tokens per sequence\n",
    "- `stride=1`: Shift window by 1 token (maximum overlap)\n",
    "- `shuffle=False`: Keep original order to see the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45e1c454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  464,  4935, 20336, 46566]]), tensor([[ 4935, 20336, 46566,   286]])]\n"
     ]
    }
   ],
   "source": [
    "# Create dataloader with small parameters for demonstration\n",
    "dataloader = create_dataloader(\n",
    "    text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "# Get the first batch\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"First batch:\")\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5fd588",
   "metadata": {},
   "source": [
    "Notice how the first batch contains:\n",
    "- A list with 2 tensors: `[input_tensor, target_tensor]`\n",
    "- Each tensor has shape `[batch_size, max_length]` = `[1, 4]`\n",
    "\n",
    "Let's get the second batch to see the sliding window effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ee80c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 4935, 20336, 46566,   286]]), tensor([[20336, 46566,   286, 43989]])]\n"
     ]
    }
   ],
   "source": [
    "# Get the second batch\n",
    "second_batch = next(data_iter)\n",
    "print(\"Second batch:\")\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f937712",
   "metadata": {},
   "source": [
    "With `stride=1`, the window shifted by just 1 token. Most tokens appear in multiple training examples, helping the model learn from more context.\n",
    "\n",
    "## 6. Batching Multiple Sequences\n",
    "\n",
    "Now let's create a more realistic dataloader:\n",
    "- `batch_size=8`: Process 8 sequences at once\n",
    "- `stride=4`: Less overlap (window shifts by entire sequence length)\n",
    "- This is more efficient for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e4d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  464,  4935, 20336, 46566],\n",
      "        [  286, 43989,   290, 38201],\n",
      "        [  198,   220,   220,   220],\n",
      "        [  220,   198,  1212, 47179],\n",
      "        [  318,   329,   262,   779],\n",
      "        [  286,  2687,  6609,   287],\n",
      "        [  262,  1578,  1829,   290],\n",
      "        [  198,  1712,   584,  3354]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 4935, 20336, 46566,   286],\n",
      "        [43989,   290, 38201,   198],\n",
      "        [  220,   220,   220,   220],\n",
      "        [  198,  1212, 47179,   318],\n",
      "        [  329,   262,   779,   286],\n",
      "        [ 2687,  6609,   287,   262],\n",
      "        [ 1578,  1829,   290,   198],\n",
      "        [ 1712,   584,  3354,   286]])\n"
     ]
    }
   ],
   "source": [
    "# Create a dataloader with batch_size=8\n",
    "dataloader = create_dataloader(text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs shape:\", inputs.shape)  # [batch_size, max_length]\n",
    "print(\"Targets shape:\", targets.shape)  # [batch_size, max_length]\n",
    "print(\"\\nInputs (token IDs):\\n\", inputs)\n",
    "print(\"\\nTargets (token IDs):\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e3834",
   "metadata": {},
   "source": [
    "Perfect! We now have:\n",
    "- **8 sequences** in the batch (rows)\n",
    "- Each sequence has **4 tokens** (columns)\n",
    "- Targets are shifted by 1 position\n",
    "\n",
    "## 7. Decoding to See Actual Text\n",
    "\n",
    "Let's decode the token IDs back to text to see what the model is actually learning from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0329d4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DECODED BATCH - Showing Input → Target pairs\n",
      "==================================================\n",
      "\n",
      "Sequence 1:\n",
      "  Input:  'The Project Gutenberg eBook'\n",
      "  Target: ' Project Gutenberg eBook of'\n",
      "\n",
      "Sequence 2:\n",
      "  Input:  ' of Romeo and Juliet'\n",
      "  Target: ' Romeo and Juliet\\n'\n",
      "\n",
      "Sequence 3:\n",
      "  Input:  '\\n   '\n",
      "  Target: '    '\n",
      "\n",
      "Sequence 4:\n",
      "  Input:  ' \\nThis ebook'\n",
      "  Target: '\\nThis ebook is'\n",
      "\n",
      "Sequence 5:\n",
      "  Input:  ' is for the use'\n",
      "  Target: ' for the use of'\n",
      "\n",
      "Sequence 6:\n",
      "  Input:  ' of anyone anywhere in'\n",
      "  Target: ' anyone anywhere in the'\n",
      "\n",
      "Sequence 7:\n",
      "  Input:  ' the United States and'\n",
      "  Target: ' United States and\\n'\n",
      "\n",
      "Sequence 8:\n",
      "  Input:  '\\nmost other parts'\n",
      "  Target: 'most other parts of'\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer for decoding\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DECODED BATCH - Showing Input → Target pairs\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Decode each sequence in the batch\n",
    "for i in range(len(inputs)):\n",
    "    input_text = tokenizer.decode(inputs[i].tolist())\n",
    "    target_text = tokenizer.decode(targets[i].tolist())\n",
    "    \n",
    "    print(f\"\\nSequence {i + 1}:\")\n",
    "    print(f\"  Input:  {repr(input_text)}\")\n",
    "    print(f\"  Target: {repr(target_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637de18",
   "metadata": {},
   "source": [
    "## Understanding the Output\n",
    "\n",
    "Notice how each target is the input shifted by one token:\n",
    "- The model learns to predict the **next token** at each position\n",
    "- For input token sequence `[A, B, C, D]`, the model predicts `[B, C, D, E]`\n",
    "- At position 0: given `A`, predict `B`\n",
    "- At position 1: given `A, B`, predict `C`\n",
    "- At position 2: given `A, B, C`, predict `D`\n",
    "- At position 3: given `A, B, C, D`, predict `E`\n",
    "\n",
    "This is called **causal language modeling** - the fundamental training objective for LLMs like GPT!\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Sliding windows** create multiple training examples from continuous text\n",
    "2. **Stride controls overlap** - smaller stride = more examples but slower\n",
    "3. **Batching** groups sequences for efficient parallel processing\n",
    "4. **Input-target pairs** are shifted by 1 token for next-token prediction\n",
    "5. **DataLoaders** handle shuffling, batching, and multi-processing automatically\n",
    "\n",
    "This data preparation pipeline is essential for training any autoregressive language model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
