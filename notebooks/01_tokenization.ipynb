{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221756e9",
   "metadata": {},
   "source": [
    "# Tokenization for LLMs: From Text to Numbers\n",
    "\n",
    "**Why Tokenization Matters**: Neural networks can only process numbers, not text. Tokenization is the crucial first step that converts human-readable text into numerical representations that language models can understand and learn from.\n",
    "\n",
    "In this tutorial, we'll build a tokenizer from scratch to understand:\n",
    "\n",
    "1. **Load and explore text data** - Working with Romeo and Juliet from Project Gutenberg\n",
    "2. **Tokenize text using regex patterns** - Breaking text into meaningful units\n",
    "3. **Build a vocabulary** - Mapping tokens to unique integer IDs\n",
    "4. **Create a BasicTokenizer class** - Encoding text to IDs and decoding back\n",
    "5. **Handle special tokens** - Dealing with unknown words and document boundaries\n",
    "6. **Compare with BPE tokenization** - Understanding modern tokenization used in GPT models\n",
    "\n",
    "By the end, you'll understand how text becomes the numerical input that powers LLMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ae168",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data\n",
    "\n",
    "First, we import the required library and load our text data - Romeo and Juliet from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d526057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "file_path = \"../data/romeo_juliet_gutenberg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "862c3452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data '../data/romeo_juliet_gutenberg.txt' loaded successfully.\n",
      "Total characters: 161,780\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(f\"Data '{file_path}' loaded successfully.\")\n",
    "print(f\"Total characters: {len(text):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be2947",
   "metadata": {},
   "source": [
    "## 2. Tokenization with Regex\n",
    "\n",
    "**The Challenge**: How do we split text into tokens? Should \"don't\" be one token or two? What about numbers and punctuation?\n",
    "\n",
    "**Our Solution**: We use a regex pattern that intelligently handles different text elements:\n",
    "\n",
    "- `[a-zA-Z]+(?:'[a-zA-Z]+)?` - Words and contractions (e.g., \"don't\", \"it's\", \"Romeo's\")\n",
    "- `[0-9]` - Individual digits (0-9) - each digit is a separate token\n",
    "- `[^\\w\\s]` - Punctuation and special characters (periods, commas, etc.)\n",
    "\n",
    "This pattern strikes a balance between granularity and meaning, keeping contractions together while separating punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1594d231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 38,250 tokens.\n",
      "\n",
      "First 30 tokens: ['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Romeo', 'and', 'Juliet', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no']\n"
     ]
    }
   ],
   "source": [
    "# Split text into tokens using regex\n",
    "# This pattern handles words, contractions, individual digits, and punctuation\n",
    "tokens = re.findall(r\"[a-zA-Z]+(?:'[a-zA-Z]+)?|[0-9]|[^\\w\\s]\", text)\n",
    "print(f\"Text split into {len(tokens):,} tokens.\")\n",
    "\n",
    "# Show a sample of tokens to see what we got\n",
    "print(f\"\\nFirst 30 tokens: {tokens[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42ff936",
   "metadata": {},
   "source": [
    "## 3. Building the Vocabulary\n",
    "\n",
    "**What is a Vocabulary?** It's a dictionary that maps each unique token to a unique integer ID. This is how we convert text to numbers!\n",
    "\n",
    "We create two mappings:\n",
    "- **`word_to_id`** - Convert tokens to IDs (for encoding: text ‚Üí numbers)\n",
    "- **`id_to_word`** - Convert IDs back to tokens (for decoding: numbers ‚Üí text)\n",
    "\n",
    "Why sort the tokens? This ensures consistent ordering across different runs, making our tokenizer deterministic and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f45ac924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4,632 unique tokens\n",
      "Reduction: 38,250 tokens ‚Üí 4,632 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# Get unique tokens and sort them alphabetically\n",
    "all_words = sorted(set(tokens))\n",
    "vocab_size = len(all_words)\n",
    "print(f\"Vocabulary size: {vocab_size:,} unique tokens\")\n",
    "print(f\"Reduction: {len(tokens):,} tokens ‚Üí {vocab_size:,} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd2fc61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token to ID mapping created.\n",
      "ID to Token mapping created.\n",
      "\n",
      "Example: 'Romeo' ‚Üí ID 663\n",
      "Example: ID 100 ‚Üí 'BUT'\n"
     ]
    }
   ],
   "source": [
    "# Create token-to-ID and ID-to-token mappings\n",
    "word_to_id = {word: i for i, word in enumerate(all_words)}\n",
    "id_to_word = {i: word for i, word in enumerate(all_words)}\n",
    "\n",
    "print(\"Token to ID mapping created.\")   \n",
    "print(\"ID to Token mapping created.\")\n",
    "print(f\"\\nExample: 'Romeo' ‚Üí ID {word_to_id.get('Romeo', 'Not found')}\")\n",
    "print(f\"Example: ID 100 ‚Üí '{id_to_word.get(100, 'Not found')}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1759bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 token mappings (sorted alphabetically):\n",
      "  '!' ‚Üí 0\n",
      "  '#' ‚Üí 1\n",
      "  '$' ‚Üí 2\n",
      "  '%' ‚Üí 3\n",
      "  '&' ‚Üí 4\n",
      "  '(' ‚Üí 5\n",
      "  ')' ‚Üí 6\n",
      "  '*' ‚Üí 7\n",
      "  ',' ‚Üí 8\n",
      "  '-' ‚Üí 9\n",
      "  '.' ‚Üí 10\n",
      "  '/' ‚Üí 11\n",
      "  '0' ‚Üí 12\n",
      "  '1' ‚Üí 13\n",
      "  '2' ‚Üí 14\n",
      "  '3' ‚Üí 15\n",
      "  '4' ‚Üí 16\n",
      "  '5' ‚Üí 17\n",
      "  '6' ‚Üí 18\n",
      "  '7' ‚Üí 19\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect some mappings to see how it works\n",
    "print(\"First 20 token mappings (sorted alphabetically):\")\n",
    "for word in all_words[:20]:\n",
    "    print(f\"  '{word}' ‚Üí {word_to_id[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acffb778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reverse mapping (ID ‚Üí Token):\n",
      "  0 ‚Üí '!'\n",
      "  1 ‚Üí '#'\n",
      "  2 ‚Üí '$'\n",
      "  3 ‚Üí '%'\n",
      "  4 ‚Üí '&'\n",
      "  5 ‚Üí '('\n",
      "  6 ‚Üí ')'\n",
      "  7 ‚Üí '*'\n",
      "  8 ‚Üí ','\n",
      "  9 ‚Üí '-'\n",
      "  10 ‚Üí '.'\n",
      "  11 ‚Üí '/'\n",
      "  12 ‚Üí '0'\n",
      "  13 ‚Üí '1'\n",
      "  14 ‚Üí '2'\n",
      "  15 ‚Üí '3'\n",
      "  16 ‚Üí '4'\n",
      "  17 ‚Üí '5'\n",
      "  18 ‚Üí '6'\n",
      "  19 ‚Üí '7'\n"
     ]
    }
   ],
   "source": [
    "# Verify reverse mapping works correctly\n",
    "print(\"Reverse mapping (ID ‚Üí Token):\")\n",
    "for i in range(20):\n",
    "    print(f\"  {i} ‚Üí '{id_to_word[i]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c50c0e",
   "metadata": {},
   "source": [
    "## 4. Adding Special Tokens\n",
    "\n",
    "**The Problem**: What happens when we encounter a word that's not in our vocabulary? Or when we need to mark where one document ends and another begins?\n",
    "\n",
    "**The Solution**: Special tokens! These are reserved tokens with specific purposes:\n",
    "\n",
    "1. **`<|unknown|>`** - Represents out-of-vocabulary (OOV) words\n",
    "   - When we see \"Hello\" (not in Romeo & Juliet), we map it to this token\n",
    "   - Preserves sentence structure even when we don't know every word\n",
    "   \n",
    "2. **`<|endoftext|>`** - Marks boundaries between different documents\n",
    "   - Essential when training on multiple texts\n",
    "   - Tells the model \"this sequence is complete, next one is unrelated\"\n",
    "\n",
    "Let's add these special tokens to our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b61c092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocabulary size: 4632\n",
      "With special tokens: 4634\n",
      "\n",
      "Special tokens added:\n",
      "  <|endoftext|> -> 4632\n",
      "  <|unknown|>   -> 4633\n"
     ]
    }
   ],
   "source": [
    "# Add special tokens to vocabulary\n",
    "all_tokens_with_special = all_words.copy()\n",
    "all_tokens_with_special.extend([\"<|endoftext|>\", \"<|unknown|>\"])\n",
    "\n",
    "# Create updated vocabulary\n",
    "vocab_with_special = {token: i for i, token in enumerate(all_tokens_with_special)}\n",
    "\n",
    "print(f\"Original vocabulary size: {len(word_to_id)}\")\n",
    "print(f\"With special tokens: {len(vocab_with_special)}\")\n",
    "print(f\"\\nSpecial tokens added:\")\n",
    "print(f\"  <|endoftext|> -> {vocab_with_special['<|endoftext|>']}\")\n",
    "print(f\"  <|unknown|>   -> {vocab_with_special['<|unknown|>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde7c2d",
   "metadata": {},
   "source": [
    "Now let's demonstrate how the `<|unknown|>` token handles words not in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bde034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to encode: 'Hello'\n",
      "Token 'Hello' ‚Üí ID: 4633\n",
      "This ID maps to: '<|unknown|>'\n",
      "\n",
      "This graceful fallback prevents errors when encountering new words!\n"
     ]
    }
   ],
   "source": [
    "# Example: Encode a word not in vocabulary\n",
    "unknown_word = \"Hello\"  # This word is not in Romeo & Juliet\n",
    "\n",
    "# Using .get() with the <|unknown|> token ID as default\n",
    "# This is a key technique: instead of raising an error, we return the unknown token ID\n",
    "unknown_token_id = vocab_with_special[\"<|unknown|>\"]\n",
    "word_id = vocab_with_special.get(unknown_word, unknown_token_id)\n",
    "\n",
    "print(f\"Attempting to encode: '{unknown_word}'\")\n",
    "print(f\"Token '{unknown_word}' ‚Üí ID: {word_id}\")\n",
    "print(f\"This ID maps to: '{all_tokens_with_special[word_id]}'\")\n",
    "print(f\"\\nThis graceful fallback prevents errors when encountering new words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9a9a9",
   "metadata": {},
   "source": [
    "## 5. Complete BasicTokenizer Class\n",
    "\n",
    "Now we'll combine everything we've learned into a reusable `BasicTokenizer` class.\n",
    "\n",
    "**Why a class?** It encapsulates all tokenization logic in one place:\n",
    "- Automatically builds vocabulary from any text corpus\n",
    "- Provides clean `encode()` and `decode()` methods\n",
    "- Handles special tokens gracefully\n",
    "- Can be easily saved, loaded, and reused\n",
    "\n",
    "**Key Design Decisions**:\n",
    "- `__init__(text)`: Creates vocabulary from the training text\n",
    "- `tokenize_text()`: Static method for splitting text (can be used independently)\n",
    "- `create_vocab()`: Static method for building vocabulary with special tokens\n",
    "- `encode()`: Convert text ‚Üí list of token IDs\n",
    "- `decode()`: Convert list of token IDs ‚Üí text (with proper punctuation spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a1a37",
   "metadata": {},
   "source": [
    "## 6. Testing the Tokenizer\n",
    "\n",
    "Let's test our complete tokenizer with different scenarios to verify it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b7f9c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"Initialize tokenizer by creating vocabulary from text with special tokens\"\"\"\n",
    "        self.word_to_id = self.create_vocab(text)\n",
    "        self.id_to_word = {i: s for s, i in self.word_to_id.items()}\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenize_text(text):\n",
    "        \"\"\"Tokenize text into tokens\"\"\"\n",
    "        tokens = re.findall(r\"[a-zA-Z]+(?:'[a-zA-Z]+)?|[0-9]|[^\\w\\s]\", text)\n",
    "        return tokens\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_vocab(text):\n",
    "        \"\"\"Create vocabulary from raw text with special tokens\"\"\"\n",
    "        # Split text into tokens using regex\n",
    "        tokens = re.findall(r\"[a-zA-Z]+(?:'[a-zA-Z]+)?|[0-9]|[^\\w\\s]\", text)\n",
    "        \n",
    "        # Get unique tokens and sort them\n",
    "        all_words = sorted(set(tokens))\n",
    "        \n",
    "        # Add special tokens at the end\n",
    "        all_words.extend([\"<|endoftext|>\", \"<|unknown|>\"])\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        vocab = {word: i for i, word in enumerate(all_words)}\n",
    "        \n",
    "        return vocab\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # Split text into tokens using the same regex pattern\n",
    "        tokens = re.findall(r\"[a-zA-Z]+(?:'[a-zA-Z]+)?|[0-9]|[^\\w\\s]\", text)\n",
    "        \n",
    "        # Convert tokens to their integer IDs\n",
    "        # Use <|unknown|> token for words not in vocabulary\n",
    "        ids = [\n",
    "            self.word_to_id.get(token, self.word_to_id[\"<|unknown|>\"]) \n",
    "            for token in tokens\n",
    "        ]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # Convert integer IDs back to tokens\n",
    "        tokens = [self.id_to_word[i] for i in ids]\n",
    "        \n",
    "        # Join tokens with spaces\n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        # Remove spaces before punctuation\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d823e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer created with vocabulary size: 4634\n"
     ]
    }
   ],
   "source": [
    "# Create the tokenizer instance\n",
    "tokenizer = BasicTokenizer(text)\n",
    "print(f\"Tokenizer created with vocabulary size: {len(tokenizer.word_to_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661a856",
   "metadata": {},
   "source": [
    "### Test 1: Handling Unknown Words (Out-of-Vocabulary)\n",
    "\n",
    "First, let's verify that our tokenizer handles words it has never seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c60dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello Romeo! This is an unknown word: xyzabc\n",
      "Decoded text:  <|unknown|> Romeo! This is an unknown word: <|unknown|>\n",
      "\n",
      "‚úì Success! Unknown words 'Hello' and 'xyzabc' were replaced with <|unknown|>\n",
      "  This allows the tokenizer to handle any text without crashing!\n"
     ]
    }
   ],
   "source": [
    "# Test with unknown words (not in Romeo & Juliet vocabulary)\n",
    "sample_text = \"Hello Romeo! This is an unknown word: xyzabc\"\n",
    "\n",
    "try:\n",
    "    encoded = tokenizer.encode(sample_text)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    \n",
    "    print(\"Original text:\", sample_text)\n",
    "    print(\"Decoded text: \", decoded)\n",
    "    print(\"\\n‚úì Success! Unknown words 'Hello' and 'xyzabc' were replaced with <|unknown|>\")\n",
    "    print(\"  This allows the tokenizer to handle any text without crashing!\")\n",
    "except KeyError as e:\n",
    "    print(f\"‚úó Error: Token '{e.args[0]}' not found in vocabulary!\")\n",
    "    print(f\"  Solution: Use .get() with <|unknown|> token as default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59595714",
   "metadata": {},
   "source": [
    "### Test 2: Basic Encoding and Decoding\n",
    "\n",
    "Let's test with text that contains words from our vocabulary to verify the encode/decode cycle works perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c02f30ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Romeo, Romeo! Wherefore art thou Romeo?\n",
      "Encoded IDs:   [663, 8, 663, 0, 890, 1095, 4152, 663, 24] ... (first 10 IDs)\n",
      "Decoded text:  Romeo, Romeo! Wherefore art thou Romeo?\n",
      "\n",
      "‚úì Perfect roundtrip! 9 tokens encoded and decoded\n",
      "  Notice punctuation spacing is preserved correctly\n"
     ]
    }
   ],
   "source": [
    "# Test with text from Romeo & Juliet (all words in vocabulary)\n",
    "sample_text = \"Romeo, Romeo! Wherefore art thou Romeo?\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Original text:\", sample_text)\n",
    "print(\"Encoded IDs:  \", encoded[:10], \"... (first 10 IDs)\")\n",
    "print(\"Decoded text: \", decoded)\n",
    "print(f\"\\n‚úì Perfect roundtrip! {len(encoded)} tokens encoded and decoded\")\n",
    "print(\"  Notice punctuation spacing is preserved correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbcd51",
   "metadata": {},
   "source": [
    "### Test 3: Numbers and Punctuation\n",
    "\n",
    "Our regex pattern treats each digit as a separate token. Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84a04811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Act 1, Scene 2: Romeo's age is 16.\n",
      "Tokens:        ['Act', '1', ',', 'Scene', '2', ':', \"Romeo's\", 'age', 'is', '1', '6', '.']\n",
      "Decoded text:  Act 1, Scene 2: <|unknown|> age is 1 6.\n",
      "\n",
      "‚úì Notice: '16' becomes two tokens ['1', '6']\n",
      "  This is intentional for our simple tokenizer!\n"
     ]
    }
   ],
   "source": [
    "# Test with numbers and special characters\n",
    "sample_text = \"Act 1, Scene 2: Romeo's age is 16.\"\n",
    "tokens_list = tokenizer.tokenize_text(sample_text)\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Original text:\", sample_text)\n",
    "print(\"Tokens:       \", tokens_list)\n",
    "print(\"Decoded text: \", decoded)\n",
    "print(f\"\\n‚úì Notice: '16' becomes two tokens ['1', '6']\")\n",
    "print(\"  This is intentional for our simple tokenizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15b374",
   "metadata": {},
   "source": [
    "### Test 4: Contractions\n",
    "\n",
    "Our regex pattern preserves contractions as single tokens, which is important for natural language understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2acd24b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: I'll go, but thou'rt not coming!\n",
      "Tokens:        [\"I'll\", 'go', ',', 'but', \"thou'rt\", 'not', 'coming', '!']\n",
      "Decoded text:  <|unknown|> go, but <|unknown|> not coming!\n",
      "\n",
      "‚úì Contractions like 'I'll' and 'thou'rt' stay together as single tokens\n"
     ]
    }
   ],
   "source": [
    "# Test with contractions\n",
    "sample_text = \"I'll go, but thou'rt not coming!\"\n",
    "tokens_list = tokenizer.tokenize_text(sample_text)\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Original text:\", sample_text)\n",
    "print(\"Tokens:       \", tokens_list)\n",
    "print(\"Decoded text: \", decoded)\n",
    "print(f\"\\n‚úì Contractions like 'I'll' and 'thou'rt' stay together as single tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee8a37",
   "metadata": {},
   "source": [
    "### Test 5: End-of-Text Token\n",
    "\n",
    "The `<|endoftext|>` token is crucial when training on multiple documents. It tells the model \"this sequence ends here, the next tokens are from a different document\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4fe4e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text: Romeo loves Juliet <|endoftext|> The sun rises in the east\n",
      "Decoded text:  Romeo loves Juliet <|unknown|> <|unknown|> <|unknown|> <|unknown|> <|unknown|> The sun <|unknown|> in the east\n",
      "\n",
      "‚úì The <|endoftext|> token marks the boundary between unrelated sequences\n",
      "  This prevents the model from learning false patterns across document boundaries\n"
     ]
    }
   ],
   "source": [
    "# Test with <|endoftext|> token to separate two different sentences\n",
    "text1 = \"Romeo loves Juliet\"\n",
    "text2 = \"The sun rises in the east\"\n",
    "combined_text = text1 + \" <|endoftext|> \" + text2\n",
    "\n",
    "encoded = tokenizer.encode(combined_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Combined text:\", combined_text)\n",
    "print(\"Decoded text: \", decoded)\n",
    "print(f\"\\n‚úì The <|endoftext|> token marks the boundary between unrelated sequences\")\n",
    "print(\"  This prevents the model from learning false patterns across document boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa31be1",
   "metadata": {},
   "source": [
    "## 7. Advanced Tokenization: Byte Pair Encoding (BPE)\n",
    "\n",
    "**Congratulations!** You've built a working tokenizer from scratch. But there's a problem...\n",
    "\n",
    "### Why Basic Word Tokenization Isn't Enough for Modern LLMs\n",
    "\n",
    "Our `BasicTokenizer` works well for learning, but has critical limitations:\n",
    "\n",
    "**Problem 1: Massive Vocabulary Size**\n",
    "- Romeo & Juliet alone: ~3,000 unique tokens\n",
    "- Full English language: 100,000+ unique words\n",
    "- GPT-3 training data: Millions of unique words!\n",
    "- **Result**: Huge memory requirements, slow lookups\n",
    "\n",
    "**Problem 2: Out-of-Vocabulary Words**\n",
    "- Every new word, typo, or name ‚Üí `<|unknown|>` token\n",
    "- Information is lost: \"xylophone\" and \"quantum\" both become `<|unknown|>`\n",
    "- **Result**: Poor handling of rare words, technical terms, proper nouns\n",
    "\n",
    "**Problem 3: No Morphological Understanding**\n",
    "- \"run\", \"running\", \"runs\", \"runner\" are completely separate tokens\n",
    "- Model doesn't learn that these words are related\n",
    "- **Result**: Inefficient learning, poor generalization\n",
    "\n",
    "### Enter Byte Pair Encoding (BPE)\n",
    "\n",
    "**The BPE Solution**: Break words into **subword units** instead of whole words!\n",
    "\n",
    "- \"running\" ‚Üí [\"run\", \"ning\"]\n",
    "- \"runner\" ‚Üí [\"run\", \"ner\"]  \n",
    "- \"uncommon\" ‚Üí [\"un\", \"common\"]\n",
    "\n",
    "**Benefits**:\n",
    "1. **Smaller vocabulary**: 30K-50K subwords (vs 100K+ words)\n",
    "2. **No true unknowns**: Any word can be built from subwords\n",
    "3. **Morphology captured**: \"run\" appears in \"running\", \"runs\", \"runner\"\n",
    "4. **Better efficiency**: Fewer parameters, faster training\n",
    "\n",
    "**Modern LLMs using BPE**: GPT-2, GPT-3, GPT-4, LLaMA, and most others!\n",
    "\n",
    "Let's explore how GPT-2's BPE tokenizer works using OpenAI's `tiktoken` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ef349e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken if needed: pip install tiktoken\n",
    "import tiktoken\n",
    "\n",
    "print(f\"tiktoken version: {tiktoken.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aecea0c",
   "metadata": {},
   "source": [
    "### Initialize GPT-2 Tokenizer\n",
    "\n",
    "GPT-2 uses BPE with a vocabulary of **50,257 tokens** - much smaller than word-level tokenizers while handling any possible text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e40c792f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 tokenizer\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"GPT-2 vocabulary size: {gpt2_tokenizer.n_vocab}\")\n",
    "\n",
    "# Note: To explore newer tokenizers like GPT-4's, you can use:\n",
    "# gpt4_tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cddfcb",
   "metadata": {},
   "source": [
    "### Encoding and Decoding with BPE\n",
    "\n",
    "Let's see how BPE handles text encoding. Notice the `allowed_special=\"all\"` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a11b3277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! <|endoftext|>\n",
      "Encoded tokens: [15496, 11, 995, 0, 220, 50256]\n",
      "Number of tokens: 6\n",
      "\n",
      "Notice how 'Hello' and 'world' are encoded as single tokens!\n",
      "BPE handles common words efficiently while breaking rare words into subwords.\n"
     ]
    }
   ],
   "source": [
    "# Encode text with special token\n",
    "sample_text = \"Hello, world! <|endoftext|>\"\n",
    "\n",
    "# Why allowed_special=\"all\"?\n",
    "# - Without it: <|endoftext|> would be tokenized as individual characters: <, |, end, of, text, |, >\n",
    "# - With it: <|endoftext|> is treated as a single special token with dedicated ID\n",
    "# This is crucial for maintaining special token semantics in the model!\n",
    "tokens = gpt2_tokenizer.encode(sample_text, allowed_special=\"all\")\n",
    "\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"Encoded tokens: {tokens}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"\\nNotice how 'Hello' and 'world' are encoded as single tokens!\")\n",
    "print(\"BPE handles common words efficiently while breaking rare words into subwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b73dd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: Hello, world! <|endoftext|>\n",
      "\n",
      "‚úì Perfect roundtrip encoding/decoding!\n"
     ]
    }
   ],
   "source": [
    "# Decode tokens back to text\n",
    "decoded_text = gpt2_tokenizer.decode(tokens)\n",
    "print(f\"Decoded text: {decoded_text}\")\n",
    "print(f\"\\n‚úì Perfect roundtrip encoding/decoding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce65747",
   "metadata": {},
   "source": [
    "### Comparing Basic vs BPE Tokenization\n",
    "\n",
    "Let's compare our `BasicTokenizer` with GPT-2's BPE tokenizer side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3464c246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOKENIZATION COMPARISON\n",
      "============================================================\n",
      "\n",
      "Text: Romeo's running towards Juliet.\n",
      "\n",
      "üìù Basic Tokenizer (Word-level):\n",
      "  Tokens: [\"Romeo's\", 'running', 'towards', 'Juliet', '.']\n",
      "  Count: 5 tokens\n",
      "\n",
      "ü§ñ BPE Tokenizer (Subword-level):\n",
      "  Token IDs: [49, 462, 78, 338, 2491, 3371, 38201, 13]\n",
      "  Count: 8 tokens\n",
      "  Decoded subwords: ['R', 'ome', 'o', \"'s\", ' running', ' towards', ' Juliet', '.']\n",
      "\n",
      "üí° Key Observations:\n",
      "  - BPE breaks 'Romeo' and 'running' into meaningful subword units\n",
      "  - This allows better handling of word variations and morphology\n",
      "  - More efficient: fewer unique tokens needed to represent any text\n"
     ]
    }
   ],
   "source": [
    "# Compare tokenization approaches on the same text\n",
    "test_text = \"Romeo's running towards Juliet.\"\n",
    "\n",
    "# Our basic tokenizer\n",
    "basic_tokens = tokenizer.tokenize_text(test_text)\n",
    "basic_encoded = tokenizer.encode(test_text)\n",
    "\n",
    "# GPT-2 BPE tokenizer\n",
    "bpe_tokens = gpt2_tokenizer.encode(test_text)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nText: {test_text}\")\n",
    "\n",
    "print(f\"\\nüìù Basic Tokenizer (Word-level):\")\n",
    "print(f\"  Tokens: {basic_tokens}\")\n",
    "print(f\"  Count: {len(basic_tokens)} tokens\")\n",
    "\n",
    "print(f\"\\nü§ñ BPE Tokenizer (Subword-level):\")\n",
    "print(f\"  Token IDs: {bpe_tokens}\")\n",
    "print(f\"  Count: {len(bpe_tokens)} tokens\")\n",
    "print(f\"  Decoded subwords: {[gpt2_tokenizer.decode([t]) for t in bpe_tokens]}\")\n",
    "\n",
    "print(f\"\\nüí° Key Observations:\")\n",
    "print(f\"  - BPE breaks 'Romeo' and 'running' into meaningful subword units\")\n",
    "print(f\"  - This allows better handling of word variations and morphology\")\n",
    "print(f\"  - More efficient: fewer unique tokens needed to represent any text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22007a7e",
   "metadata": {},
   "source": [
    "## Summary: Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Basic Tokenization** \n",
    "   - ‚úÖ Simple and intuitive to understand\n",
    "   - ‚úÖ Great for learning tokenization concepts\n",
    "   - ‚úÖ Works well for small, controlled vocabularies\n",
    "   - ‚ùå Huge vocabulary size for real applications (100K+ tokens)\n",
    "   - ‚ùå Cannot handle unseen words effectively\n",
    "   - ‚ùå No understanding of word relationships\n",
    "\n",
    "2. **BPE Tokenization (GPT-2, GPT-3, GPT-4)**\n",
    "   - ‚úÖ Smaller, efficient vocabulary (30K-50K tokens)\n",
    "   - ‚úÖ Handles any possible text through subword decomposition\n",
    "   - ‚úÖ Captures morphological relationships (run ‚Üí running ‚Üí runner)\n",
    "   - ‚úÖ Better generalization to rare and new words\n",
    "   - ‚úÖ Industry standard for modern LLMs\n",
    "\n",
    "### The Tokenization Pipeline\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "   ‚Üì\n",
    "Tokenization (splitting)\n",
    "   ‚Üì\n",
    "Vocabulary Mapping (token ‚Üí ID)\n",
    "   ‚Üì\n",
    "Token IDs (numbers for the model)\n",
    "   ‚Üì\n",
    "Model Training/Inference\n",
    "   ‚Üì\n",
    "Token IDs (output)\n",
    "   ‚Üì\n",
    "Decoding (ID ‚Üí token ‚Üí text)\n",
    "   ‚Üì\n",
    "Generated Text\n",
    "```\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "- **Use Basic Word Tokenization**: \n",
    "  - Educational purposes and learning\n",
    "  - Small, fixed domains with limited vocabulary\n",
    "  - When interpretability is crucial\n",
    "\n",
    "- **Use BPE (or similar subword methods)**:\n",
    "  - Production LLMs and real-world applications\n",
    "  - When vocabulary size matters\n",
    "  - For handling diverse, open-domain text\n",
    "  - **This is what you should use for building actual LLMs!**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you understand tokenization, the next step is learning how to:\n",
    "- Create efficient data loaders for training\n",
    "- Handle batching and sequence padding\n",
    "- Implement attention mechanisms that process these tokens\n",
    "\n",
    "Ready to move on? Check out the next notebook on data sampling and loading! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0d19c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
