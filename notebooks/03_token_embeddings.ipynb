{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e4dd42",
   "metadata": {},
   "source": [
    "# Token Embeddings\n",
    "\n",
    "In this notebook, we'll explore how Large Language Models (LLMs) convert tokens into numerical vectors called **embeddings**. We'll cover:\n",
    "\n",
    "1. **Token Embeddings**: Converting token IDs into dense vectors\n",
    "2. **Positional Embeddings**: Encoding position information\n",
    "3. **Input Embeddings**: Combining token and positional embeddings\n",
    "\n",
    "Let's start by importing the required libraries and loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cfc9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to the path so we can import from core\n",
    "blog_dir = Path.cwd().parent\n",
    "sys.path.insert(0, str(blog_dir))\n",
    "\n",
    "file_path = \"../data/romeo_juliet_gutenberg.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "75dc4972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data '../data/romeo_juliet_gutenberg.txt' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the text data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(f\"Data '{file_path}' loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a78780",
   "metadata": {},
   "source": [
    "## Part 1: Simple Token Embeddings (Toy Example)\n",
    "\n",
    "Before working with GPT-2 scale models, let's understand embeddings with a simple example.\n",
    "\n",
    "**What are Token Embeddings?**\n",
    "- Token embeddings convert discrete token IDs into continuous vector representations\n",
    "- Each token gets mapped to a learned vector of fixed dimension\n",
    "- Similar tokens will have similar embeddings after training\n",
    "\n",
    "Let's create a toy vocabulary of 6 tokens and map them to 3-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c242ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding layer: 6 tokens → 3D vectors\n"
     ]
    }
   ],
   "source": [
    "# Example: A simple sequence of 4 token IDs\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "# Define our toy vocabulary and embedding dimensions\n",
    "toy_vocab_size = 6        # Total number of tokens in vocabulary (0-5)\n",
    "toy_embedding_dim = 3     # Each token will be represented by a 3D vector\n",
    "\n",
    "# Create an embedding layer\n",
    "# This creates a lookup table: toy_vocab_size rows x toy_embedding_dim columns\n",
    "toy_embedding_layer = torch.nn.Embedding(toy_vocab_size, toy_embedding_dim)\n",
    "\n",
    "print(f\"Created embedding layer: {toy_vocab_size} tokens → {toy_embedding_dim}D vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b24a9a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding weight matrix (6 tokens × 3 dimensions):\n",
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "\n",
      "Shape: torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# View the embedding weight matrix\n",
    "# Each row represents the embedding vector for one token\n",
    "print(\"Embedding weight matrix (6 tokens × 3 dimensions):\")\n",
    "print(toy_embedding_layer.weight)\n",
    "print(f\"\\nShape: {toy_embedding_layer.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9e504a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for token ID 3:\n",
      "tensor([[-1.5919,  0.8597, -0.6055]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Embed a single token (token ID = 3)\n",
    "# This retrieves the 4th row (index 3) from the embedding matrix\n",
    "single_token_embedding = toy_embedding_layer(torch.tensor([2]))\n",
    "print(\"Embedding for token ID 3:\")\n",
    "print(single_token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb2be7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings for sequence [2, 3, 5, 1]:\n",
      "tensor([[-1.5919,  0.8597, -0.6055],\n",
      "        [ 0.3278, -1.2925,  2.9336],\n",
      "        [ 0.7666,  2.5459, -1.1468],\n",
      "        [-0.3735, -1.1398, -0.2094]], grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Shape: torch.Size([4, 3]) → (sequence_length=4, embedding_dim=3)\n"
     ]
    }
   ],
   "source": [
    "# Embed our sequence of 4 tokens\n",
    "# Each token ID is replaced with its corresponding 3D vector\n",
    "toy_token_embeddings = toy_embedding_layer(input_ids)\n",
    "print(\"Embeddings for sequence [2, 3, 5, 1]:\")\n",
    "print(toy_token_embeddings)\n",
    "print(f\"\\nShape: {toy_token_embeddings.shape} → (sequence_length=4, embedding_dim=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77559a3",
   "metadata": {},
   "source": [
    "## Part 2: GPT-2 Scale Token Embeddings\n",
    "\n",
    "Now let's work with GPT-2's actual vocabulary and embedding dimensions.\n",
    "\n",
    "**GPT-2 Specifications:**\n",
    "- Vocabulary size: 50,257 tokens\n",
    "- Embedding dimension: 768 (for GPT-2 base) - we'll use 256 for this tutorial\n",
    "- The embedding layer learns a unique vector for each token during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "215969ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 vocabulary size: 50,257\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 tokenizer to understand the vocabulary size\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"GPT-2 vocabulary size: {gpt2_tokenizer.n_vocab:,}\")\n",
    "\n",
    "# Note: To explore newer tokenizers like GPT-4's, you can use:\n",
    "# gpt4_tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07752a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer: 50,257 tokens → 256D vectors\n",
      "Total parameters: 12,865,792\n"
     ]
    }
   ],
   "source": [
    "# Create GPT-2 scale token embedding layer\n",
    "GPT2_VOCAB_SIZE = 50257      # GPT-2's vocabulary size\n",
    "GPT2_EMBEDDING_DIM = 256     # Embedding dimension (GPT-2 small uses 768, we use 256 for tutorial)\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(GPT2_VOCAB_SIZE, GPT2_EMBEDDING_DIM)\n",
    "print(f\"Token embedding layer: {GPT2_VOCAB_SIZE:,} tokens → {GPT2_EMBEDDING_DIM}D vectors\")\n",
    "print(f\"Total parameters: {GPT2_VOCAB_SIZE * GPT2_EMBEDDING_DIM:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8288e793",
   "metadata": {},
   "source": [
    "### Create a Data Batch\n",
    "\n",
    "Let's create a batch of tokenized sequences to work with. We'll use a context length of 4 tokens and batch size of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6045a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([8, 4]) → (batch_size=8, sequence_length=4)\n"
     ]
    }
   ],
   "source": [
    "#Import the data loader we created earlier\n",
    "from core.data_loader import create_dataloader\n",
    "\n",
    "\n",
    "# Define sequence parameters\n",
    "CONTEXT_LENGTH = 4  # Maximum sequence length (number of tokens)\n",
    "BATCH_SIZE = 8      # Number of sequences to process together\n",
    "\n",
    "# Create dataloader with our text\n",
    "dataloader = create_dataloader(\n",
    "    text, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    max_length=CONTEXT_LENGTH,\n",
    "    stride=CONTEXT_LENGTH,  # No overlap between sequences\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get one batch of data\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(f\"Batch shape: {inputs.shape} → (batch_size={BATCH_SIZE}, sequence_length={CONTEXT_LENGTH})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0df2149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[  464,  4935, 20336, 46566],\n",
      "        [  286, 43989,   290, 38201],\n",
      "        [  198,   220,   220,   220],\n",
      "        [  220,   198,  1212, 47179],\n",
      "        [  318,   329,   262,   779],\n",
      "        [  286,  2687,  6609,   287],\n",
      "        [  262,  1578,  1829,   290],\n",
      "        [  198,  1712,   584,  3354]])\n",
      "\n",
      "Inputs shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "# Examine the token IDs in our batch\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(f\"\\nInputs shape: {inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c33a26",
   "metadata": {},
   "source": [
    "### Convert Token IDs to Token Embeddings\n",
    "\n",
    "Now we'll convert each token ID into its corresponding embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "103a3a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape: torch.Size([8, 4, 256])\n",
      "  → (batch_size=8, sequence_length=4, embedding_dim=256)\n",
      "\n",
      "Each of the 8 sequences has 4 tokens,\n",
      "and each token is now a 256-dimensional vector.\n"
     ]
    }
   ],
   "source": [
    "# Apply token embedding layer to our batch\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
    "print(f\"  → (batch_size={BATCH_SIZE}, sequence_length={CONTEXT_LENGTH}, embedding_dim={GPT2_EMBEDDING_DIM})\")\n",
    "print(f\"\\nEach of the {BATCH_SIZE} sequences has {CONTEXT_LENGTH} tokens,\")\n",
    "print(f\"and each token is now a {GPT2_EMBEDDING_DIM}-dimensional vector.\")\n",
    "\n",
    "# Uncomment to see the actual embedding values (they're randomly initialized)\n",
    "# print(\"\\nToken embeddings:\\n\", token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1559f3",
   "metadata": {},
   "source": [
    "## Part 3: Positional Embeddings\n",
    "\n",
    "**Problem:** Token embeddings alone don't capture word order!\n",
    "- The embeddings for \"dog bites man\" would be identical to \"man bites dog\"\n",
    "\n",
    "**Solution:** Add positional embeddings\n",
    "- Each position (0, 1, 2, 3, ...) gets its own learnable embedding\n",
    "- These are added to token embeddings to inject position information\n",
    "\n",
    "Let's create positional embeddings for our sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b59ec153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embedding layer: 4 positions → 256D vectors\n",
      "This means we can handle sequences up to 4 tokens long\n"
     ]
    }
   ],
   "source": [
    "# Create positional embedding layer\n",
    "# We need one embedding for each possible position in our context\n",
    "pos_embedding_layer = torch.nn.Embedding(CONTEXT_LENGTH, GPT2_EMBEDDING_DIM)\n",
    "\n",
    "print(f\"Positional embedding layer: {CONTEXT_LENGTH} positions → {GPT2_EMBEDDING_DIM}D vectors\")\n",
    "print(f\"This means we can handle sequences up to {CONTEXT_LENGTH} tokens long\")\n",
    "\n",
    "# Uncomment to see the positional embedding weights\n",
    "# print(\"\\nPositional embedding weights:\")\n",
    "# print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ae5be0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position indices: tensor([0, 1, 2, 3])\n",
      "Positional embeddings shape: torch.Size([4, 256])\n",
      "  → (sequence_length=4, embedding_dim=256)\n"
     ]
    }
   ],
   "source": [
    "# Generate positional embeddings for positions [0, 1, 2, 3]\n",
    "position_indices = torch.arange(CONTEXT_LENGTH)\n",
    "pos_embeddings = pos_embedding_layer(position_indices)\n",
    "\n",
    "print(f\"Position indices: {position_indices}\")\n",
    "print(f\"Positional embeddings shape: {pos_embeddings.shape}\")\n",
    "print(f\"  → (sequence_length={CONTEXT_LENGTH}, embedding_dim={GPT2_EMBEDDING_DIM})\")\n",
    "\n",
    "# Uncomment to see the actual positional embedding values\n",
    "# print(\"\\nPositional embeddings:\\n\", pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb5097",
   "metadata": {},
   "source": [
    "## Part 4: Combining Token and Positional Embeddings\n",
    "\n",
    "The final input embeddings = token embeddings + positional embeddings\n",
    "\n",
    "**Broadcasting in PyTorch:**\n",
    "- `token_embeddings`: shape `[8, 4, 256]` (batch, sequence, embedding)\n",
    "- `pos_embeddings`: shape `[4, 256]` (sequence, embedding)\n",
    "- PyTorch automatically broadcasts pos_embeddings across the batch dimension\n",
    "- Result: shape `[8, 4, 256]`\n",
    "\n",
    "This means every sequence in the batch gets the same positional encoding added to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fbc478b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape:      torch.Size([8, 4, 256])\n",
      "Positional embeddings shape: torch.Size([4, 256])\n",
      "Input embeddings shape:      torch.Size([8, 4, 256])\n",
      "\n",
      "✓ Broadcasting worked! Positional embeddings added to each sequence in the batch.\n"
     ]
    }
   ],
   "source": [
    "# Combine token and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(f\"Token embeddings shape:      {token_embeddings.shape}\")\n",
    "print(f\"Positional embeddings shape: {pos_embeddings.shape}\")\n",
    "print(f\"Input embeddings shape:      {input_embeddings.shape}\")\n",
    "print(f\"\\n✓ Broadcasting worked! Positional embeddings added to each sequence in the batch.\")\n",
    "\n",
    "# Uncomment to see the final input embeddings\n",
    "# print(\"\\nInput embeddings:\\n\", input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69975e2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "\n",
    "1. **Token Embeddings**: Convert discrete token IDs → continuous vectors\n",
    "2. **Positional Embeddings**: Encode position information for each token\n",
    "3. **Input Embeddings**: Token + Positional embeddings = what the model actually processes\n",
    "\n",
    "**Key dimensions for GPT-2:**\n",
    "- Vocabulary size: 50,257 tokens\n",
    "- Embedding dimension: 256 (in this tutorial; GPT-2 uses 768)\n",
    "- Context length: 4 tokens (GPT-2 uses 1024)\n",
    "\n",
    "These input embeddings are now ready to be fed into the transformer's attention layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24dca0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
